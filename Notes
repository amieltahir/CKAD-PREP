REPLICATION CONTROLLERS

-------

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    name: my-app
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
  spec:
    containers:
      - name: nginx-container
        image: nginx

replicas: 3     

------------

REPLICA SET

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rc
  labels:
    name: my-app
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
  spec:
    containers:
      - name: nginx-container
        image: nginx

replicas: 3     
selector: 
  matchLabels:
    type: front-end

-----

kubectl create -f replicaset-definition.yaml 
kubectl get replicaset
kubectl delete replicaset myapp-replicaset ** also deletes all underlying pods
kubectl replace -f replicaset-definition.yaml
kubectl scale --replicas=6 -f replicaset-definition.yam;
kubectl scale --replicas=6 replicaset myapp-replicaset

----


kubectl [command] [TYPE] [NAME] -o <output_format>

Here are some of the commonly used formats:

-o jsonOutput a JSON formatted API object.
-o namePrint only the resource name and nothing else.
-o wideOutput in the plain-text format with any additional information.
-o yamlOutput a YAML formatted API object.

----

NAMESPACES

create -f namespace-dev.yaml 
create namespace dev

kubectl config set-context $(kubectl config get current-context) --namespace=dev 
kubectl get namespace --all-namespaces 

kubectl run pod redis --image redis --namespace=finance

------

While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.

Before we begin, familiarize yourself with the two options that can come in handy while working with the below commands:

--dry-run: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on the screen.



Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4



Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

----------------

kubectl expose pod redis --port=6379 --name redis-service
kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
kubectl run custom-nginx --image nginx --port=8080
kubectl create deployment redis-deploy --image redis --replicas=2 --namespace=dev-ns

Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

kubectl run httpd --image=httpd:alpine --port=80 --expose

-----------------------

build, modify container images

Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. 

---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"

------------


Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.


-------------

env:
  - name: APP_COLOR
    value: pink

env:
  - name: APP_COLOR
    valueFrom: 
         configMapKeyRef:
env:
  - name: APP_COLOR
    valueFrom: 
         secretKeyRef:

--------------------------

CONFIG MAP

create config map --> kubectl create configmap 
or

imperative

kubectl create configmap <config-name> --from-literal=<key>=<value> 
kubectl create configmap <app-config> --from-literal=APP_COLOR=blue --from-literal=APP_MOD=prod

declarative 

kubectl create configmap <config-name> --from-file=<path_to_file>
kubectl create configmap <app-config> --from-file=app_config.properties 

config-map.yaml 

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

------

config map in pods

1. ENV

envFrom:
  - configMapRef:
        name: app-config

2. SINGLE ENV

env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: APP_COLOR

3. volume

volumes:
- name: app-config-volume
  configMap:
    name: app-config

----

Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color

----------------------------------------------

SECRETS

1. imperative 

1.  kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASS=passwd
2.  kubectl create secret generic secret-name --from-file=<path_to_file> 
3.  kubectl create secret --from-file=app_secret.properties 

2. Declarative 

apiVersion: v1
kind: Secret
metadata:
  name: app-secret 
data:
  DB_HOST: mysql
  DB_USER: root
  DB_PASSWORD: passwd 


kubectl create -f secret-data.yaml  

ENCODE THE PASSWORD DATA 
echo -n 'mysql' | base64  >> enconded value
echo -n 'root' | base64 >> encoded value
echo -n 'passwd' | base64 >> enconded value

if you want to view secret 

kubectl get secrets 
kubectl describe secrets
kubectl get secret app-secret -o YAML >> view encoded value

decode secrets

echo -n "&4jd" | base64 --decode
echo -n "JDhd" | base64 --decode
echo -n "DKFJ" | base64 --decode


inject secret in pod 

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret

--------------------


https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

---

multi container pods

apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command:
      - sleep
      - "1000"
  - image: redis
    name: gold

---

Edit the pod in the elastic-stack namespace to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.

Name: app

Container Name: sidecar

Container Image: kodekloud/filebeat-configured

Volume Mount: log-volume

Mount Path: /var/log/event-simulator/

Existing Container Name: app

Existing Container Image: kodekloud/event-simulator


---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate

---

Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds

Pod: red

initContainer Configured Correctly

---
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"

---------------------------------------------

Readiness & Liveness Probes

POD Status -- pending, container_creating,running --> kubectl get pods

POD Conditions --  kubectl describe pod

Pod scheduled true, false
Initialized   true, false
ContainersReady  true, false
Ready           true, false

the pod takes a long time to be ready so readiness probe ensure its ready for service

readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 8

readinessProbe:
  tcpSocket:
    port: 3306 

readinessProbe:
  exec:
    command:
      - cat
      - /app/is_ready


LivenessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 8

LivenessProbe:
  tcpSocket:
    port: 3306 

LivenessProbe:
  exec:
    command:
      - cat
      - /app/is_ready
--------------


Rolling updates & rollbacks in deployments


kubectl create -f deployment-definition.yml 
kubectl get deployments
kubectl apply -f deployment-definition.yml 
kubectl set image deployment/my-app-deployment nginx=nginx:1.9.1
kubectl rollout status deployment/my-app-deployment
kubectl rollout history deployment/my-app-deployment
kubectl rollout undo deployment/my-app-deployment 

kubectl rollout history deployment nginx --revision=1
kubectl set image deployment nginx nginx=nginx:1.17 --record

controlplane $ kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true


strategy:
    type: Recreate

strategy:
    type: RollingUpdate

-----------
A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.

Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.

The frontend-v2 deployment currently has 2 replicas. The frontend service now routes traffic to 7 pods in total ( 5 replicas on the front deployment and 2 replicas from frontend-v2 deployment).

Since the service distributes traffic to all pods equally, in this case, approximately 29% of the traffic will go to frontend-v2 deploymnt.

To reduce this below 20%, scale down the pods on the v2 version to the minimum possible replicas = 1.

Run: kubectl scale deployment --replicas=1 frontend-v2

Once this is done, only ~17% of traffic should go to the v2 version.

kubectl scale deployment --replicas=1 frontend-v2 
deployment.apps/frontend-v2 scaled

controlplane ~ âžœ  kubectl scale deployment --replicas=0 frontend
deployment.apps/frontend scaled

------------------------------
JOBS & CRON JOBS

restartPolicy: always, never, onFaliure

apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  template:
    spec:
       containers:
          - name: math-add
            image: nginx
            command: ['expr', '3', '+','2'] 
       restartPolicy: Never


controlplane $ kubectl logs math-add-job-mkxjd math-add
5


apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  template:
    spec:
       containers:
          - name: random-error
            image: kodecloud/random-error
          
       restartPolicy: Never

apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  parrallelism: 3
  template:
    spec:
       containers:
          - name: random-error
            image: kodecloud/random-error

-------
cron jobs

apiVersion: batch/v1
kind: CronJob
metadata:
  name: reporting-cron-job
spec:
  schedule: "*/1 * * * * "
  jobTemplate:
    spec:
      template:
        spec:
           containers:
              - name: reporting-tool
                image: reporting-tool
           restartPolicy: Never


------

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15
  template:
    spec:
      containers:
        - name: throw-dice
          image: kodekloud/throw-dice
      restartPolicy: Never


apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 2
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  parallelism: 3
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never


apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: 30 21 * * *
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25
      template:
        spec:
          containers:
            - name: throw-dice
              image: kodekloud/throw-dice
          restartPolicy: Never

-------------------------------------------------------
You are requested to make the new application available at /pay.


Identify and implement the best approach to making this application available on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

-----------------------------------------------------

CREATING NODE PORT SERVICES

Usage:
  kubectl create service nodeport NAME [--tcp=port:targetPort]
[--dry-run=server|client|none] [options]

---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort

----------------


kubectl expose pod --help
Expose a resource as a new Kubernetes service.

 Looks up a deployment, service, replica set, replication controller or pod by
name and uses the selector for that resource as the selector for a new service
on the specified port. A deployment or replica set will be exposed as a service
only if its selector is convertible to a selector that service supports, i.e.
when the selector contains only the matchLabels component. Note that if no port
is specified via --port and the exposed resource has multiple ports, all will be
re-used by the new service. Also if no labels are specified, the new service
will re-use the labels from the resource it exposes.

 Possible resources include (case insensitive):

 pod (po), service (svc), replicationcontroller (rc), deployment (deploy),
replicaset (rs)

Examples:
  # Create a service for a replicated nginx, which serves on port 80 and
connects to the containers on port 8000
  kubectl expose rc nginx --port=80 --target-port=8000
  
  # Create a service for a replication controller identified by type and name
specified in "nginx-controller.yaml", which serves on port 80 and connects to
the containers on port 8000
  kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000
  
  # Create a service for a pod valid-pod, which serves on port 444 with the name
"frontend"
  kubectl expose pod valid-pod --port=444 --name=frontend
  
  # Create a second service based on the above service, exposing the container
port 8443 as port 443 with the name "nginx-https"
  kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https
  
  # Create a service for a replicated streaming application on port 4100
balancing UDP traffic and named 'video-stream'.
  kubectl expose rc streamer --port=4100 --protocol=UDP --name=video-stream
  
  # Create a service for a replicated nginx using replica set, which serves on
port 80 and connects to the containers on port 8000
  kubectl expose rs nginx --port=80 --target-port=8000
  
  # Create a service for an nginx deployment, which serves on port 80 and
connects to the containers on port 8000
  kubectl expose deployment nginx --port=80 --target-port=8000

Options:
    --allow-missing-template-keys=true:
        If true, ignore any errors in templates when a field or map key is
        missing in the template. Only applies to golang and jsonpath output
        formats.

    --cluster-ip='':
        ClusterIP to be assigned to the service. Leave empty to auto-allocate,
        or set to 'None' to create a headless service.

    --dry-run='none':
        Must be "none", "server", or "client". If client strategy, only print
        the object that would be sent, without sending it. If server strategy,
        submit server-side request without persisting the resource.

    --external-ip='':
        Additional external IP address (not managed by Kubernetes) to accept
        for the service. If this IP is routed to a node, the service can be
        accessed by this IP in addition to its generated service IP.

    --field-manager='kubectl-expose':
        Name of the manager used to track field ownership.

    -f, --filename=[]:
        Filename, directory, or URL to files identifying the resource to
        expose a service

    -k, --kustomize='':
        Process the kustomization directory. This flag can't be used together
        with -f or -R.

    -l, --labels='':
        Labels to apply to the service created by this call.

    --load-balancer-ip='':
        IP to assign to the LoadBalancer. If empty, an ephemeral IP will be
        created and used (cloud-provider specific).

    --name='':
        The name for the newly created object.

    -o, --output='':
        Output format. One of: (json, yaml, name, go-template,
        go-template-file, template, templatefile, jsonpath, jsonpath-as-json,
        jsonpath-file).

    --override-type='merge':
        The method used to override the generated object: json, merge, or
        strategic.

    --overrides='':
        An inline JSON override for the generated object. If this is
        non-empty, it is used to override the generated object. Requires that
        the object supply a valid apiVersion field.

    --port='':
        The port that the service should serve on. Copied from the resource
        being exposed, if unspecified

    --protocol='':
        The network protocol for the service to be created. Default is 'TCP'.

    -R, --recursive=false:
        Process the directory used in -f, --filename recursively. Useful when
        you want to manage related manifests organized within the same
        directory.

    --save-config=false:
        If true, the configuration of current object will be saved in its
        annotation. Otherwise, the annotation will be unchanged. This flag is
        useful when you want to perform kubectl apply on this object in the
        future.

    --selector='':
        A label selector to use for this service. Only equality-based selector
        requirements are supported. If empty (the default) infer the selector
        from the replication controller or replica set.)

    --session-affinity='':
        If non-empty, set the session affinity for the service to this; legal
        values: 'None', 'ClientIP'

    --show-managed-fields=false:
        If true, keep the managedFields when printing objects in JSON or YAML
        format.

    --target-port='':
        Name or number for the port on the container that the service should
        direct traffic to. Optional.

    --template='':
        Template string or path to template file to use when -o=go-template,
        -o=go-template-file. The template format is golang templates
        [http://golang.org/pkg/text/template/#pkg-overview].

    --type='':
        Type for this service: ClusterIP, NodePort, LoadBalancer, or
        ExternalName. Default is 'ClusterIP'.

Usage:
  kubectl expose (-f FILENAME | TYPE NAME) [--port=port]
[--protocol=TCP|UDP|SCTP] [--target-port=number-or-name] [--name=name]
[--external-ip=external-ip-of-service] [--type=type] [options]
