REPLICATION CONTROLLERS

-------

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    name: my-app
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
  spec:
    containers:
      - name: nginx-container
        image: nginx

replicas: 3     

------------

REPLICA SET

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rc
  labels:
    name: my-app
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
  spec:
    containers:
      - name: nginx-container
        image: nginx

replicas: 3     
selector: 
  matchLabels:
    type: front-end

-----

kubectl create -f replicaset-definition.yaml 
kubectl get replicaset
kubectl delete replicaset myapp-replicaset ** also deletes all underlying pods
kubectl replace -f replicaset-definition.yaml
kubectl scale --replicas=6 -f replicaset-definition.yam;
kubectl scale --replicas=6 replicaset myapp-replicaset

----


kubectl [command] [TYPE] [NAME] -o <output_format>

Here are some of the commonly used formats:

-o jsonOutput a JSON formatted API object.
-o namePrint only the resource name and nothing else.
-o wideOutput in the plain-text format with any additional information.
-o yamlOutput a YAML formatted API object.

----

NAMESPACES

create -f namespace-dev.yaml 
create namespace dev

kubectl config set-context $(kubectl config get current-context) --namespace=dev 
kubectl get namespace --all-namespaces 

kubectl run pod redis --image redis --namespace=finance

------

While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.

Before we begin, familiarize yourself with the two options that can come in handy while working with the below commands:

--dry-run: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on the screen.



Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4



Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

----------------

kubectl expose pod redis --port=6379 --name redis-service
kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
kubectl run custom-nginx --image nginx --port=8080
kubectl create deployment redis-deploy --image redis --replicas=2 --namespace=dev-ns

Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

kubectl run httpd --image=httpd:alpine --port=80 --expose

-----------------------

build, modify container images

Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. 

---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"

------------


Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.


-------------

env:
  - name: APP_COLOR
    value: pink

env:
  - name: APP_COLOR
    valueFrom: 
         configMapKeyRef:
env:
  - name: APP_COLOR
    valueFrom: 
         secretKeyRef:

--------------------------

CONFIG MAP

create config map --> kubectl create configmap 
or

imperative

kubectl create configmap <config-name> --from-literal=<key>=<value> 
kubectl create configmap <app-config> --from-literal=APP_COLOR=blue --from-literal=APP_MOD=prod

declarative 

kubectl create configmap <config-name> --from-file=<path_to_file>
kubectl create configmap <app-config> --from-file=app_config.properties 

config-map.yaml 

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

------

config map in pods

1. ENV

envFrom:
  - configMapRef:
        name: app-config

2. SINGLE ENV

env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: APP_COLOR

3. volume

volumes:
- name: app-config-volume
  configMap:
    name: app-config

----

Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color

----------------------------------------------

SECRETS

1. imperative 

1.  kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASS=passwd
2.  kubectl create secret generic secret-name --from-file=<path_to_file> 
3.  kubectl create secret --from-file=app_secret.properties 

2. Declarative 

apiVersion: v1
kind: Secret
metadata:
  name: app-secret 
data:
  DB_HOST: mysql
  DB_USER: root
  DB_PASSWORD: passwd 


kubectl create -f secret-data.yaml  

ENCODE THE PASSWORD DATA 
echo -n 'mysql' | base64  >> enconded value
echo -n 'root' | base64 >> encoded value
echo -n 'passwd' | base64 >> enconded value

if you want to view secret 

kubectl get secrets 
kubectl describe secrets
kubectl get secret app-secret -o YAML >> view encoded value

decode secrets

echo -n "&4jd" | base64 --decode
echo -n "JDhd" | base64 --decode
echo -n "DKFJ" | base64 --decode


inject secret in pod 

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret

--------------------


https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

---

multi container pods

apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command:
      - sleep
      - "1000"
  - image: redis
    name: gold

---

Edit the pod in the elastic-stack namespace to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.

Name: app

Container Name: sidecar

Container Image: kodekloud/filebeat-configured

Volume Mount: log-volume

Mount Path: /var/log/event-simulator/

Existing Container Name: app

Existing Container Image: kodekloud/event-simulator


---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate

---

Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds

Pod: red

initContainer Configured Correctly

---
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"

---------------------------------------------

Readiness & Liveness Probes

POD Status -- pending, container_creating,running --> kubectl get pods

POD Conditions --  kubectl describe pod

Pod scheduled true, false
Initialized   true, false
ContainersReady  true, false
Ready           true, false

the pod takes a long time to be ready so readiness probe ensure its ready for service

readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 8

readinessProbe:
  tcpSocket:
    port: 3306 

readinessProbe:
  exec:
    command:
      - cat
      - /app/is_ready


LivenessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 8

LivenessProbe:
  tcpSocket:
    port: 3306 

LivenessProbe:
  exec:
    command:
      - cat
      - /app/is_ready
--------------


Rolling updates & rollbacks in deployments


kubectl create -f deployment-definition.yml 
kubectl get deployments
kubectl apply -f deployment-definition.yml 
kubectl set image deployment/my-app-deployment nginx=nginx:1.9.1
kubectl rollout status deployment/my-app-deployment
kubectl rollout history deployment/my-app-deployment
kubectl rollout undo deployment/my-app-deployment 

kubectl rollout history deployment nginx --revision=1
kubectl set image deployment nginx nginx=nginx:1.17 --record

controlplane $ kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true


strategy:
    type: Recreate

strategy:
    type: RollingUpdate

-----------
A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.

Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.

The frontend-v2 deployment currently has 2 replicas. The frontend service now routes traffic to 7 pods in total ( 5 replicas on the front deployment and 2 replicas from frontend-v2 deployment).

Since the service distributes traffic to all pods equally, in this case, approximately 29% of the traffic will go to frontend-v2 deploymnt.

To reduce this below 20%, scale down the pods on the v2 version to the minimum possible replicas = 1.

Run: kubectl scale deployment --replicas=1 frontend-v2

Once this is done, only ~17% of traffic should go to the v2 version.

kubectl scale deployment --replicas=1 frontend-v2 
deployment.apps/frontend-v2 scaled

controlplane ~ âžœ  kubectl scale deployment --replicas=0 frontend
deployment.apps/frontend scaled

------------------------------
JOBS & CRON JOBS

restartPolicy: always, never, onFaliure

apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  template:
    spec:
       containers:
          - name: math-add
            image: nginx
            command: ['expr', '3', '+','2'] 
       restartPolicy: Never


controlplane $ kubectl logs math-add-job-mkxjd math-add
5


apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  template:
    spec:
       containers:
          - name: random-error
            image: kodecloud/random-error
          
       restartPolicy: Never

apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  parrallelism: 3
  template:
    spec:
       containers:
          - name: random-error
            image: kodecloud/random-error

-------
cron jobs

apiVersion: batch/v1
kind: CronJob
metadata:
  name: reporting-cron-job
spec:
  schedule: "*/1 * * * * "
  jobTemplate:
    spec:
      template:
        spec:
           containers:
              - name: reporting-tool
                image: reporting-tool
           restartPolicy: Never


------

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15
  template:
    spec:
      containers:
        - name: throw-dice
          image: kodekloud/throw-dice
      restartPolicy: Never


apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 2
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  parallelism: 3
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never


piVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: 30 21 * * *
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25
      template:
        spec:
          containers:
            - name: throw-dice
              image: kodekloud/throw-dice
          restartPolicy: Never

-------------------------------------------------------
